{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740f9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer는 텍스트 데이터를 숫자 벡터로 변환하는 도구입니다.\n",
    "# scikit-learn의 feature_extraction.text 모듈에 들어있고, \n",
    "# TF-IDF(Term Frequency - Inverse Document Frequency) 방식을 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4327a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 사전: ['공부합니다' '나는' '배우고' '응용합니다' '자연어' '재미있습니다' '처리는' '처리를']\n",
      "TF-IDF 행렬 크기: (3, 8)\n",
      "[[0.5844829  0.5844829  0.         0.         0.34520502 0.\n",
      "  0.         0.44451431]\n",
      " [0.         0.         0.         0.         0.38537163 0.65249088\n",
      "  0.65249088 0.        ]\n",
      " [0.         0.         0.5844829  0.5844829  0.34520502 0.\n",
      "  0.         0.44451431]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 샘플 문서들\n",
    "docs = [\n",
    "    \"나는 자연어 처리를 공부합니다\",\n",
    "    \"자연어 처리는 재미있습니다\",\n",
    "    \"자연어 처리를 배우고 응용합니다\"\n",
    "]\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(\"단어 사전:\", vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF 행렬 크기:\", X.shape)\n",
    "\n",
    "# 희소 행렬 → 밀집 행렬 변환\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b4edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 인덱스: {'<OOV>': 1, '나는': 2, '밥을': 3, '먹었다': 4, '학교에': 5, '갔다': 6}\n",
      "훈련 시퀀스: [[2, 3, 4], [5, 6]]\n",
      "검증 시퀀스: [[3, 1, 4]]\n",
      "훈련 패딩 결과:\n",
      " [[2 3 4 0 0 0 0 0 0 0]\n",
      " [5 6 0 0 0 0 0 0 0 0]]\n",
      "검증 패딩 결과:\n",
      " [[3 1 4 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 1. 훈련/검증 데이터 준비\n",
    "X_train = [\"나는 밥을 먹었다\", \"학교에 갔다\"]\n",
    "X_valid = [\"밥을 많이 먹었다\"]\n",
    "\n",
    "# 2. 토크나이저 정의 (상위 10000개 단어만 사용 예시)\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "\n",
    "# 3. 훈련 데이터로 단어 사전 구축\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# 4. 단어 인덱스 확인\n",
    "print(\"단어 인덱스:\", tokenizer.word_index)\n",
    "\n",
    "# 5. 텍스트 → 시퀀스 변환\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_valid_seq = tokenizer.texts_to_sequences(X_valid)\n",
    "\n",
    "print(\"훈련 시퀀스:\", X_train_seq)\n",
    "print(\"검증 시퀀스:\", X_valid_seq)           # 검증 시퀀스: [[3, 1, 4]]   # '많이'는 사전에 없어 <OOV>(1) 처리됨\n",
    "\n",
    "# 6. 패딩 처리 (모델 입력 크기 맞추기)\n",
    "max_len = 10  # 원하는 최대 길이\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_valid_pad = pad_sequences(X_valid_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "print(\"훈련 패딩 결과:\\n\", X_train_pad)\n",
    "print(\"검증 패딩 결과:\\n\", X_valid_pad)\n",
    "\n",
    "\n",
    "# 이렇게 하면 X_train_pad, X_valid_pad를 바로 딥러닝 모델의 입력 데이터로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8629f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
